{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into a Rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/bin/spark-2.0.1')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "trainRdd = sc.textFile('input/tr.csv').cache()\n",
    "testRdd = sc.textFile('input/test.csv').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def makeDF(rdd, data_set='train'):\n",
    "    #Save the row in the rdd that contains the column names, then delete it from the rdd.\n",
    "    header = rdd.first()\n",
    "    header_rdd = rdd.filter(lambda l: 'margin' in l)\n",
    "    rdd_no_header = rdd.subtract(header_rdd)\n",
    "\n",
    "    #Prepare the schema and rdd to build the dataframe. \n",
    "    #Species will be a string while all columns will be floats.\n",
    "    #Note if data_set == 'test', the won't have a species column\n",
    "    fields = [StructField(field_name, FloatType(), True) for field_name in header.split(\",\")]   \n",
    "    if data_set == 'train':\n",
    "        fields[1].dataType = StringType()\n",
    "        rdd_split = rdd_no_header.map(lambda l: l.split(\",\")).map(lambda l: [float(x) if i != 1 else str(x) for i,x in enumerate(l)])\n",
    "    else:\n",
    "        rdd_split = rdd_no_header.map(lambda l: l.split(\",\")).map(lambda l: [float(x) for i,x in enumerate(l)])\n",
    "\n",
    "    schema=StructType(fields)\n",
    "    \n",
    "    return spark.createDataFrame(rdd_split, schema).cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = makeDF(trainRdd)\n",
    "test_df = makeDF(testRdd, data_set='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Stage to encode our target variable.\n",
    "stInd = StringIndexer(inputCol='species', outputCol='speciesEnc')\n",
    "\n",
    "\n",
    "cols = train_df.columns\n",
    "cols.remove('species')\n",
    "cols.remove('id')\n",
    "# Stage to create a single feature column for the Spark API.\n",
    "va = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "#Simple random forest model.\n",
    "rf = RandomForestClassifier(labelCol=\"speciesEnc\", \n",
    "                            predictionCol=\"prediction\",\n",
    "                            probabilityCol=\"probability\",\n",
    "                            rawPredictionCol=\"rawPrediction\",\n",
    "                            numTrees=10,\n",
    "                            featureSubsetStrategy=\"auto\")\n",
    "\n",
    "pipeline = Pipeline(stages=[stInd, va, rf])\n",
    "pipe_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pipeline to make predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_df = pipe_model.transform(test_df).cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "species_labels = pipe_model.stages[0].labels\n",
    "species_labels.insert(0,'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decimal import *\n",
    "pred_rdd = prediction_df.select('id', 'probability').rdd.map(lambda x: [x[0]] + [Decimal(a) for a in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sol_fields = [StructField(field_name, DecimalType(38, 38), True) for field_name in species_labels]\n",
    "sol_fields[0].dataType = FloatType()\n",
    "sol_schema=StructType(sol_fields)    \n",
    "pred_df = spark.createDataFrame(pred_rdd, sol_schema).cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
